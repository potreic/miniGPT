{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Utilities"
      ],
      "metadata": {
        "id": "6bAfqbZAf1fv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20e392d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05832b74-8474-44e7-bf16-89cc21db07b7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d88a259"
      },
      "source": [
        "Once your drive is mounted, you can navigate to the directory where your data is stored."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7e72b33e"
      },
      "source": [
        "input_path = '/content/drive/MyDrive/nlp/shakespeare.txt'\n",
        "output_path = '/content/drive/MyDrive/nlp/shakespeare_char'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "3HAVL2UpuhB6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class Module(ABC):\n",
        "    def __init__(self):\n",
        "        self.training = True\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, x):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def backward(self, grad):\n",
        "        pass\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return self.forward(*args, **kwargs)\n"
      ],
      "metadata": {
        "id": "7pG27uyNf3kL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Activations\n",
        "\n",
        "class Softmax(Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.cache_output = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        X_shifted = X - np.max(X, axis=-1, keepdims=True)  # (*, d) softmax trick for numerical stability\n",
        "        exp_X = np.exp(X_shifted)  # (*, d)\n",
        "        softmax_output = exp_X / np.sum(exp_X, axis=-1, keepdims=True)  # (*, d)\n",
        "        self.cache_output = softmax_output\n",
        "        return softmax_output\n",
        "\n",
        "    def backward(self, dZ_or_Y_true, Y_true=None):\n",
        "        if Y_true is not None:\n",
        "            Y_hat = self.cache_output\n",
        "            Y = np.zeros_like(Y_hat)\n",
        "            Y[np.arange(Y_true.size), Y_true] = 1\n",
        "            # CrossEntropy + Softmax derivative simplifies beautifully to:\n",
        "            # Note: PyTorch's cross_entropy averages over batch, so we need to divide by batch size\n",
        "            batch_size = Y_hat.shape[0]\n",
        "            return (Y_hat - Y) / batch_size  # ∂(CE○Softmax)/∂x = (ŷ - y) / N  (see: https://www.parasdahal.com/softmax-crossentropy)\n",
        "        else:\n",
        "            # Otherwise, compute gradient using the full Softmax Jacobian to backpropagate through softmax outputs\n",
        "            # (see: https://tombolton.io/2018/08/25/softmax-back-propagation-solved-i-think/)\n",
        "            dZ = dZ_or_Y_true\n",
        "            softmax_output = self.cache_output\n",
        "            # Softmax Jacobian: ∂σ_i/∂x_j = σ_i(δ_ij - σ_j)\n",
        "            return softmax_output * (dZ - np.sum(dZ * softmax_output, axis=-1, keepdims=True))  # ∂L/∂x_i = σ_i(∂L/∂σ_i - Σ_j ∂L/∂σ_j·σ_j)\n",
        "\n",
        "    def params(self):\n",
        "        return {}\n",
        "\n",
        "    def grads(self):\n",
        "        return {}\n",
        "\n",
        "\n",
        "class ReLU(Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.cache_input = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.cache_input = X\n",
        "        return np.maximum(0, X)\n",
        "\n",
        "    def backward(self, dZ):\n",
        "        X = self.cache_input\n",
        "        # ReLU derivative: 1 if x > 0, else 0\n",
        "        grad = (X > 0).astype(np.float32)  # ∂ReLU/∂x = 1 if x > 0, else 0\n",
        "        return dZ * grad  # ∂L/∂x = ∂L/∂ReLU · ∂ReLU/∂x\n",
        "\n",
        "    def params(self):\n",
        "        return {}\n",
        "\n",
        "    def grads(self):\n",
        "        return {}\n",
        "\n",
        "\n",
        "class LeakyReLU(Module):\n",
        "    def __init__(self, alpha=0.01):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.cache_input = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.cache_input = X\n",
        "        return np.where(X > 0, X, self.alpha * X)\n",
        "\n",
        "    def backward(self, dZ):\n",
        "        X = self.cache_input\n",
        "        # LeakyReLU derivative: 1 if x > 0, else α\n",
        "        grad = np.where(X > 0, 1.0, self.alpha)  # ∂LeakyReLU/∂x = 1 if x > 0, else α\n",
        "        return dZ * grad  # ∂L/∂x = ∂L/∂LeakyReLU · ∂LeakyReLU/∂x\n",
        "\n",
        "    def params(self):\n",
        "        return {}\n",
        "\n",
        "    def grads(self):\n",
        "        return {}"
      ],
      "metadata": {
        "id": "p7e4L8SIj0VG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title cross entropy loss\n",
        "def cross_entropy_loss(logits, targets, eps=1e-7):\n",
        "    N = targets.shape[0]\n",
        "\n",
        "    # logits -> log_softmax via log-sum-exp trick\n",
        "    max_logits = np.max(logits, axis=-1, keepdims=True)\n",
        "    shifted = logits - max_logits\n",
        "    log_softmax = shifted - np.log(np.sum(np.exp(shifted), axis=-1, keepdims=True) + eps)\n",
        "\n",
        "    # negative log-likelihood: -log P(correct_class)\n",
        "    nll = -log_softmax[np.arange(N), targets]\n",
        "\n",
        "    return np.mean(nll)"
      ],
      "metadata": {
        "id": "75WKpWTLx-xj"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenizer"
      ],
      "metadata": {
        "id": "_Xj1cyDwJAph"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "q6tbCX4NyFew"
      },
      "outputs": [],
      "source": [
        "#@title WordTokenizer\n",
        "\n",
        "import re\n",
        "\n",
        "class WordTokenizer:\n",
        "    def __init__(self, min_freq=1, max_vocab_size=None):\n",
        "        self.min_freq = min_freq\n",
        "        self.max_vocab_size = max_vocab_size\n",
        "        self.words = []\n",
        "        self.stoi = {}\n",
        "        self.itos = {}\n",
        "\n",
        "    def build_vocab(self, text):\n",
        "        # Replace special whitespace characters with tokens\n",
        "        text = text.replace('\\n', ' <|newline|> ')\n",
        "        text = text.replace('\\t', ' <|tab|> ')\n",
        "        text = text.replace('\\r', ' <|carriage_return|> ')\n",
        "\n",
        "        # Tokenize text into:\n",
        "        # 1. Special tokens like <|newline|>\n",
        "        # 2. Words (alphanumeric/underscores)\n",
        "        # 3. Punctuation or non-word chars\n",
        "        tokens = re.findall(r\"<\\|[^|]+\\|>|\\b\\w+\\b|[^\\s\\w]\", text)\n",
        "        tokens = [\n",
        "            token.lower() if not (token.startswith('<|') and token.endswith('|>')) else token\n",
        "            for token in tokens\n",
        "        ]\n",
        "\n",
        "        # Count word frequencies\n",
        "        word_counts = {}\n",
        "        for token in tokens:\n",
        "            word_counts[token] = word_counts.get(token, 0) + 1\n",
        "\n",
        "        # Filter by min_freq\n",
        "        filtered_words = [word for word, count in word_counts.items() if count >= self.min_freq]\n",
        "\n",
        "        # Sort by frequency (desc) then alphabetically\n",
        "        sorted_words = sorted(filtered_words, key=lambda x: (-word_counts[x], x))\n",
        "\n",
        "        # Limit vocab size if needed (reserve 4 for special tokens)\n",
        "        if self.max_vocab_size:\n",
        "            sorted_words = sorted_words[:self.max_vocab_size - 4]\n",
        "\n",
        "        # Final vocab list\n",
        "        self.words = ['<pad>', '<unk>', '<bos>', '<eos>'] + sorted_words\n",
        "        self.stoi = {word: i for i, word in enumerate(self.words)}\n",
        "        self.itos = {i: word for i, word in enumerate(self.words)}\n",
        "\n",
        "    def encode(self, text, add_bos=True, add_eos=True):\n",
        "        # Apply same replacements\n",
        "        text = text.replace('\\n', ' <|newline|> ')\n",
        "        text = text.replace('\\t', ' <|tab|> ')\n",
        "        text = text.replace('\\r', ' <|carriage_return|> ')\n",
        "\n",
        "        tokens = re.findall(r\"<\\|[^|]+\\|>|\\b\\w+\\b|[^\\s\\w]\", text)\n",
        "        tokens = [\n",
        "            token.lower() if not (token.startswith('<|') and token.endswith('|>')) else token\n",
        "            for token in tokens\n",
        "        ]\n",
        "\n",
        "        indices = [self.stoi.get(token, 1) for token in tokens]  # 1 = <unk>\n",
        "\n",
        "        if add_bos:\n",
        "            indices = [2] + indices  # <bos>\n",
        "        if add_eos:\n",
        "            indices = indices + [3]  # <eos>\n",
        "        return indices\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        words = [self.itos[t] for t in tokens if t in self.itos]\n",
        "\n",
        "        # Remove special tokens\n",
        "        for special in ['<bos>', '<eos>', '<pad>', '<unk>']:\n",
        "            while special in words:\n",
        "                words.remove(special)\n",
        "\n",
        "        result = []\n",
        "        for i, word in enumerate(words):\n",
        "            if word == '<|newline|>':\n",
        "                result.append('\\n')\n",
        "            elif word == '<|tab|>':\n",
        "                result.append('\\t')\n",
        "            elif word == '<|carriage_return|>':\n",
        "                result.append('\\r')\n",
        "            else:\n",
        "                result.append(word)\n",
        "                if i < len(words) - 1:\n",
        "                    next_word = words[i + 1]\n",
        "                    if not next_word.startswith('<|') and next_word not in '\\'.,;!?)\"':\n",
        "                        result.append(' ')\n",
        "\n",
        "        text = ''.join(result)\n",
        "        text = re.sub(r' +', ' ', text)\n",
        "        text = re.sub(r\" ([\\'.,;:!?)])\", r'\\1', text)\n",
        "        text = re.sub(r\"\\b([A-Za-z]+) ?' ?(ll|re|ve|d|s|t|m)\\b\", r\"\\1'\\2\", text)\n",
        "        text = re.sub(r\"([a-z]) :\", r'\\1:', text)\n",
        "        text = re.sub(r' *\\n *', '\\n', text)\n",
        "        text = re.sub(r' *\\t *', '\\t', text)\n",
        "        text = re.sub(r' *\\r *', '\\r', text)\n",
        "        return text.strip()\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return len(self.words)\n",
        "\n",
        "    @property\n",
        "    def eos_token_id(self):\n",
        "        return 3\n",
        "\n",
        "    @property\n",
        "    def bos_token_id(self):\n",
        "        return 2\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title BPE Tokenizer\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "class BPETokenizer:\n",
        "    def __init__(self, vocab_size=1000):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.stoi = {}\n",
        "        self.itos = {}\n",
        "        self.merges = []\n",
        "        self.merge_ranks = {}\n",
        "\n",
        "    def _get_word_freqs(self, texts):\n",
        "        if isinstance(texts, str):\n",
        "            texts = [texts]\n",
        "\n",
        "        word_freqs = Counter()\n",
        "        for text in texts:\n",
        "            text = text.replace('\\n', ' <|newline|> ')\n",
        "            text = text.replace('\\t', ' <|tab|> ')\n",
        "            text = text.replace('\\r', ' <|carriage_return|> ')\n",
        "\n",
        "            # Tokenize text into:\n",
        "            # 1. Special tokens with format <|...|>   -> <\\|[^|]+\\|>\n",
        "            # 2. Words (alphanumerics/underscores)    -> \\b\\w+\\b\n",
        "            # 3. Punctuation/non-word characters      -> [^\\s\\w]\n",
        "            tokens = re.findall(r\"<\\|[^|]+\\|>|\\b\\w+\\b|[^\\s\\w]\", text)\n",
        "            tokens = [token if not (token.startswith('<|') and token.endswith('|>')) else token for token in tokens]\n",
        "\n",
        "            for token in tokens:\n",
        "                word_freqs[token] += 1\n",
        "        return word_freqs\n",
        "\n",
        "    def _get_pairs(self, word):\n",
        "        pairs = set()\n",
        "        prev_char = word[0]\n",
        "        for char in word[1:]:\n",
        "            pairs.add((prev_char, char))\n",
        "            prev_char = char\n",
        "        return pairs\n",
        "\n",
        "    def _merge_word(self, word_tokens, pair):\n",
        "        new_tokens = []\n",
        "        i = 0\n",
        "        while i < len(word_tokens):\n",
        "            # If we find the pair, merge it\n",
        "            if i < len(word_tokens) - 1 and word_tokens[i] == pair[0] and word_tokens[i + 1] == pair[1]:\n",
        "                new_tokens.append(pair[0] + pair[1])\n",
        "                i += 2  # Skip both tokens\n",
        "            else:\n",
        "                new_tokens.append(word_tokens[i])\n",
        "                i += 1\n",
        "        return new_tokens\n",
        "\n",
        "    def build_vocab(self, texts):\n",
        "        word_freqs = self._get_word_freqs(texts)\n",
        "\n",
        "        # Step 1: Initialize with character-level tokens\n",
        "        vocab = {}\n",
        "        base_chars = set()\n",
        "\n",
        "        for word, freq in word_freqs.items():\n",
        "            if word.startswith('<|') and word.endswith('|>'):\n",
        "                # Special tokens stay as single units\n",
        "                vocab[word + ' </w>'] = freq\n",
        "                base_chars.add(word)\n",
        "            else:\n",
        "                # Split into characters + end-of-word marker\n",
        "                char_list = list(word) + ['</w>']\n",
        "                vocab[' '.join(char_list)] = freq\n",
        "                base_chars.update(word)\n",
        "\n",
        "        base_chars.add('</w>')\n",
        "\n",
        "        # Step 2: Calculate how many merges we can do\n",
        "        special_tokens = ['<pad>', '<unk>', '<bos>', '<eos>']\n",
        "        base_vocab_size = len(special_tokens) + len(base_chars)\n",
        "        available_for_merges = max(0, self.vocab_size - base_vocab_size)\n",
        "\n",
        "        # Step 3: Iteratively find and apply merges\n",
        "        for i in range(available_for_merges):\n",
        "            pairs = defaultdict(int)\n",
        "            # Count all adjacent pairs across all words\n",
        "            for word, freq in vocab.items():\n",
        "                word_pairs = self._get_pairs(word.split())\n",
        "                for pair in word_pairs:\n",
        "                    if not any(p.startswith('<|') and '|>' in p for p in pair):\n",
        "                        pairs[pair] += freq\n",
        "\n",
        "            if not pairs:\n",
        "                break\n",
        "\n",
        "            # Find most frequent pair, that will be merged in this iteration\n",
        "            best_pair = max(pairs, key=pairs.get)\n",
        "\n",
        "            # Update vocab with the merged pair\n",
        "            new_vocab = {}\n",
        "            for word, freq in vocab.items():\n",
        "                word_tokens = word.split()\n",
        "                merged_tokens = self._merge_word(word_tokens, best_pair)\n",
        "                new_vocab[' '.join(merged_tokens)] = freq\n",
        "\n",
        "            vocab = new_vocab\n",
        "\n",
        "            self.merges.append(best_pair)\n",
        "            self.merge_ranks[best_pair] = i  # Remember when this was learned for encoding and decoding\n",
        "\n",
        "         # Step 4: Add special tokens, base chars to the vocab and pairs to the vocab\n",
        "        all_tokens = special_tokens + sorted(list(base_chars))\n",
        "        for pair in self.merges:\n",
        "            all_tokens.append(''.join(pair))\n",
        "\n",
        "        all_tokens = all_tokens[:self.vocab_size]\n",
        "\n",
        "        self.stoi = {token: idx for idx, token in enumerate(all_tokens)}\n",
        "        self.itos = {idx: token for idx, token in enumerate(all_tokens)}\n",
        "        self.vocab_size = len(self.stoi)\n",
        "\n",
        "    def _tokenize_word(self, word):\n",
        "        if word.startswith('<|') and word.endswith('|>'):\n",
        "            return [word, '</w>']\n",
        "\n",
        "        word_tokens = list(word) + ['</w>']\n",
        "\n",
        "        while len(word_tokens) > 1:\n",
        "            pairs = self._get_pairs(word_tokens)\n",
        "            if not pairs:\n",
        "                break\n",
        "\n",
        "            best_pair = min(pairs, key=lambda pair: self.merge_ranks.get(pair, float('inf')))\n",
        "            if best_pair not in self.merge_ranks:\n",
        "                break\n",
        "\n",
        "            word_tokens = self._merge_word(word_tokens, best_pair)\n",
        "\n",
        "        return word_tokens\n",
        "\n",
        "    def encode(self, text, add_bos=False, add_eos=False):\n",
        "        text = text.replace('\\n', ' <|newline|> ')\n",
        "        text = text.replace('\\t', ' <|tab|> ')\n",
        "        text = text.replace('\\r', ' <|carriage_return|> ')\n",
        "\n",
        "        tokens = re.findall(r\"<\\|[^|]+\\|>|\\b\\w+\\b|[^\\s\\w]\", text)\n",
        "        tokens = [token if not (token.startswith('<|') and token.endswith('|>')) else token for token in tokens]\n",
        "\n",
        "        indices = []\n",
        "        if add_bos:\n",
        "            indices.append(2)  # <bos> index\n",
        "\n",
        "        for token in tokens:\n",
        "            word_tokens = self._tokenize_word(token)\n",
        "            for word_token in word_tokens:\n",
        "                indices.append(self.stoi.get(word_token, 1))  # 1 is <unk>\n",
        "\n",
        "        if add_eos:\n",
        "            indices.append(3)  # <eos> index\n",
        "\n",
        "        return indices\n",
        "\n",
        "    def decode(self, indices):\n",
        "        tokens = [self.itos.get(idx, '<unk>') for idx in indices]\n",
        "\n",
        "        # Remove special tokens\n",
        "        for special in ['<bos>', '<eos>', '<pad>', '<unk>']:\n",
        "            while special in tokens:\n",
        "                tokens.remove(special)\n",
        "\n",
        "        result = []\n",
        "\n",
        "        for i, token in enumerate(tokens):\n",
        "            if token == '<|newline|>':\n",
        "                result.append('\\n')\n",
        "            elif token == '<|tab|>':\n",
        "                result.append('\\t')\n",
        "            elif token == '<|carriage_return|>':\n",
        "                result.append('\\r')\n",
        "            else:\n",
        "                # Check if token ends with </w> (end of word marker)\n",
        "                if token.endswith('</w>'):\n",
        "                    word = token[:-len('</w>')]   # Remove '</w>'\n",
        "                    result.append(word)\n",
        "\n",
        "                    if i < len(tokens) - 1:\n",
        "                        result.append(' ')\n",
        "                else:\n",
        "                    # it is a subword token\n",
        "                    result.append(token)\n",
        "\n",
        "        text = ''.join(result)\n",
        "        text = re.sub(r' +', ' ', text)\n",
        "        text = re.sub(r\" ([\\'.,;:!?)])\", r'\\1', text)\n",
        "        text = re.sub(r\"\\b([A-Za-z]+) ?' ?(ll|re|ve|d|s|t|m)\\b\", r\"\\1'\\2\", text)\n",
        "        text = re.sub(r\"([a-z]) :\", r'\\1:', text)\n",
        "        text = re.sub(r' *\\n *', '\\n', text)\n",
        "        text = re.sub(r' *\\t *', '\\t', text)\n",
        "        text = re.sub(r' *\\r *', '\\r', text)\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    @property\n",
        "    def eos_token_id(self):\n",
        "        return 3\n",
        "\n",
        "    @property\n",
        "    def bos_token_id(self):\n",
        "        return 2"
      ],
      "metadata": {
        "id": "oICOi_-z1kyb"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Embedding"
      ],
      "metadata": {
        "id": "SrkY2ehfgJkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        self.W = np.random.randn(vocab_size, embed_dim) * 0.02\n",
        "        self.dW = None\n",
        "        self.cache_input = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.cache_input = X  # (B, T)\n",
        "        out = self.W[X]  # (B, T, embed_dim)\n",
        "        return out\n",
        "\n",
        "    def backward(self, dZ):\n",
        "        self.dW = np.zeros_like(self.W)\n",
        "        X = self.cache_input\n",
        "        np.add.at(self.dW, X.flatten(), dZ.reshape(-1, self.embed_dim))  # in place accumulation for efficiency\n",
        "\n",
        "    def params(self):\n",
        "        return {\"W\": self.W}\n",
        "\n",
        "    def grads(self):\n",
        "        return {\"W\": self.dW}"
      ],
      "metadata": {
        "id": "6gAZJyRDgLO7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Positional Encoding\n",
        "\n",
        "learned positional encoding"
      ],
      "metadata": {
        "id": "XwasfsOLgc7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(Module):\n",
        "    def __init__(self, max_len, d_model):\n",
        "        super().__init__()\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.W = np.random.randn(max_len, d_model) * 0.02\n",
        "        self.dW = None\n",
        "        self.cache_input = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.cache_input = X\n",
        "        B, T, C = X.shape\n",
        "\n",
        "        pos_emb = self.W[:T, :]  # (T, C)\n",
        "        out = X + pos_emb  # (B, T, C)\n",
        "        return out\n",
        "\n",
        "    def backward(self, dZ):\n",
        "        B, T, C = dZ.shape\n",
        "        self.dW = np.zeros_like(self.W)\n",
        "\n",
        "        # out = X + W[:T, :], so ∂out/∂W[t] = 1 for every batch element at position t\n",
        "        # ∂L/∂W[t] = sum over all batch gradients at position t\n",
        "        self.dW[:T, :] = np.sum(dZ, axis=0)\n",
        "        return dZ\n",
        "\n",
        "    def params(self):\n",
        "        return {\"W\": self.W}\n",
        "\n",
        "    def grads(self):\n",
        "        return {\"W\": self.dW}"
      ],
      "metadata": {
        "id": "-_z616kUgUiM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Layer Normalization"
      ],
      "metadata": {
        "id": "EAEa0y6tiJE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(Module):\n",
        "    def __init__(self, d_model, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.eps = eps\n",
        "\n",
        "        self.gamma = np.ones(d_model)\n",
        "        self.beta = np.zeros(d_model)\n",
        "\n",
        "        self.dgamma = None\n",
        "        self.dbeta = None\n",
        "        self.cache = {}\n",
        "\n",
        "    def forward(self, X):\n",
        "        mean = np.mean(X, axis=-1, keepdims=True)  # (B, T, 1)\n",
        "        var = np.var(X, axis=-1, keepdims=True)  # (B, T, 1)\n",
        "\n",
        "        X_norm = (X - mean) / np.sqrt(var + self.eps)  # z-score normalization\n",
        "\n",
        "        out = self.gamma * X_norm + self.beta  # (B, T, C)\n",
        "\n",
        "        self.cache = {\n",
        "            'X': X,\n",
        "            'mean': mean,\n",
        "            'var': var,\n",
        "            'X_norm': X_norm\n",
        "        }\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dZ):\n",
        "        X = self.cache['X']\n",
        "        mean = self.cache['mean']\n",
        "        var = self.cache['var']\n",
        "        X_norm = self.cache['X_norm']\n",
        "\n",
        "        N = X.shape[-1]\n",
        "\n",
        "        # sum over batch and time dimensions\n",
        "        self.dgamma = np.sum(dZ * X_norm, axis=(0, 1))  # mul -> ∂L/∂γ = Σ ∂L/∂y · x̂\n",
        "        self.dbeta = np.sum(dZ, axis=(0, 1))  # sum -> ∂L/∂β = Σ ∂L/∂y\n",
        "\n",
        "        dX_norm = dZ * self.gamma  # mul -> ∂L/∂x̂ = ∂L/∂y · γ\n",
        "\n",
        "        # x̂ = (x-μ)/√(σ²+ε), so ∂x̂/∂σ² = (x-μ)·(-1/2)(σ²+ε)^(-3/2)\n",
        "        dvar = np.sum(dX_norm * (X - mean), axis=-1, keepdims=True) * -1/2 * (var + self.eps)**(-3/2)\n",
        "\n",
        "        # μ = (1/N)Σx, so ∂μ/∂x = 1/N\n",
        "        # μ affects x̂ directly: x̂ = (x-μ)/√(σ²+ε), so ∂x̂/∂μ = -1/√(σ²+ε)\n",
        "        # μ affects σ² indirectly: σ² = (1/N)Σ(x-μ)², so ∂σ²/∂μ = (1/N)Σ(-2(x-μ)) = -2(x-μ)\n",
        "        dmean = np.sum(dX_norm, axis=-1, keepdims=True) * -1 / np.sqrt(var + self.eps) + \\\n",
        "            dvar * np.sum(-2 * (X - mean), axis=-1, keepdims=True) / N\n",
        "\n",
        "        # x̂ = (x - μ) / √(σ² + ε), so ∂x̂/∂x = 1 / √(σ² + ε)\n",
        "        # μ = (1/N) Σx, so ∂μ/∂x = 1/N\n",
        "        # σ² = (1/N) Σ(x - μ)², so ∂σ²/∂x = 2(x - μ)/N\n",
        "        # Total: ∂L/∂x = ∂L/∂x̂ · ∂x̂/∂x + ∂L/∂μ · ∂μ/∂x + ∂L/∂σ² · ∂σ²/∂x\n",
        "        dX = dX_norm / np.sqrt(var + self.eps) + dmean / N + dvar * 2 * (X - mean) / N\n",
        "\n",
        "        return dX\n",
        "\n",
        "    def params(self):\n",
        "        return {\"gamma\": self.gamma, \"beta\": self.beta}\n",
        "\n",
        "    def grads(self):\n",
        "        return {\"gamma\": self.dgamma, \"beta\": self.dbeta}"
      ],
      "metadata": {
        "id": "7xp3VX8FiO0N"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Multi Head Attention"
      ],
      "metadata": {
        "id": "zWP_FFsRja03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(Module):\n",
        "    \"\"\"\n",
        "    Multi-Head Attention (Attention is All You Need <3): https://arxiv.org/abs/1706.03762\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads\n",
        "\n",
        "        self.W_q = Linear(d_model, d_model)\n",
        "        self.W_k = Linear(d_model, d_model)\n",
        "        self.W_v = Linear(d_model, d_model)\n",
        "        self.W_o = Linear(d_model, d_model)\n",
        "        self.softmax = Softmax()\n",
        "\n",
        "        self.cache = {}\n",
        "\n",
        "    def forward(self, X, mask=None):\n",
        "        B, T, C = X.shape\n",
        "\n",
        "        Q = self.W_q(X)  # (B, T, C)\n",
        "        K = self.W_k(X)  # (B, T, C)\n",
        "        V = self.W_v(X)  # (B, T, C)\n",
        "\n",
        "        Q = Q.reshape(B, T, self.n_heads, self.d_k).swapaxes(1, 2)  # (B, nh, T, d_k)\n",
        "        K = K.reshape(B, T, self.n_heads, self.d_k).swapaxes(1, 2)  # (B, nh, T, d_k)\n",
        "        V = V.reshape(B, T, self.n_heads, self.d_k).swapaxes(1, 2)  # (B, nh, T, d_k)\n",
        "\n",
        "        # scaled dot product\n",
        "        scores = Q @ K.swapaxes(-2, -1) / np.sqrt(self.d_k)  # (B, nh, T, T)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores + mask  # (B, nh, T, T)\n",
        "\n",
        "        original_shape = scores.shape\n",
        "        scores_reshaped = scores.reshape(-1, scores.shape[-1])  # (B*nh*T, T)\n",
        "        # scaled dot product with softmax\n",
        "        attn_weights_reshaped = self.softmax(scores_reshaped)  # (B*nh*T, T)\n",
        "        attn_weights = attn_weights_reshaped.reshape(original_shape)  # (B, nh, T, T)\n",
        "\n",
        "        attn_output = attn_weights @ V  # (B, nh, T, d_k)\n",
        "\n",
        "        attn_output = attn_output.swapaxes(1, 2).reshape(B, T, C)  # (B, T, C)\n",
        "        output = self.W_o(attn_output)  # (B, T, C)\n",
        "\n",
        "        self.cache = {\n",
        "            'X': X, 'Q': Q, 'K': K, 'V': V,\n",
        "            'attn_weights': attn_weights,\n",
        "            'original_shape': original_shape\n",
        "        }\n",
        "\n",
        "        return output\n",
        "\n",
        "    def backward(self, dZ):\n",
        "        X, Q, K, V = self.cache['X'], self.cache['Q'], self.cache['K'], self.cache['V']\n",
        "        attn_weights = self.cache['attn_weights']\n",
        "        original_shape = self.cache['original_shape']\n",
        "        B, T, C = X.shape\n",
        "\n",
        "        dattn_output = self.W_o.backward(dZ)  # ∂L/∂attn_output\n",
        "        dattn_output = dattn_output.reshape(B, T, self.n_heads, self.d_k).swapaxes(1, 2)  # (B, nh, T, d_k)\n",
        "\n",
        "        # Attention output = attn_weights @ V\n",
        "        # ∂L/∂attn_weights = ∂L/∂attn_output @ V^T\n",
        "        # ∂L/∂V = attn_weights^T @ ∂L/∂attn_output\n",
        "        dattn_weights = dattn_output @ V.swapaxes(-2, -1)  # ∂L/∂attn_weights\n",
        "        dV = attn_weights.swapaxes(-2, -1) @ dattn_output  # ∂L/∂V\n",
        "\n",
        "        dattn_weights_reshaped = dattn_weights.reshape(-1, dattn_weights.shape[-1])\n",
        "        dscores_reshaped = self.softmax.backward(dattn_weights_reshaped)\n",
        "        dscores = dscores_reshaped.reshape(original_shape)\n",
        "\n",
        "        # scores = QK^T / √d_k\n",
        "        # ∂L/∂Q = ∂L/∂scores @ K / √d_k\n",
        "        # ∂L/∂K = Q^T @ ∂L/∂scores / √d_k\n",
        "        dQ = dscores @ K / np.sqrt(self.d_k)  # ∂L/∂Q\n",
        "        dK = Q.swapaxes(-2, -1) @ dscores / np.sqrt(self.d_k)  # ∂L/∂K\n",
        "\n",
        "        dQ = dQ.swapaxes(1, 2).reshape(B, T, C)\n",
        "        dK = dK.swapaxes(1, 2).reshape(B, T, C)\n",
        "        dV = dV.swapaxes(1, 2).reshape(B, T, C)\n",
        "\n",
        "        # X branches into Q, K, V - sum gradients from all paths\n",
        "        dX_q = self.W_q.backward(dQ)  # ∂L/∂X via Q\n",
        "        dX_k = self.W_k.backward(dK)  # ∂L/∂X via K\n",
        "        dX_v = self.W_v.backward(dV)  # ∂L/∂X via V\n",
        "\n",
        "        return dX_q + dX_k + dX_v  # ∂L/∂X = sum of all paths\n",
        "\n",
        "    def params(self):\n",
        "        params = {}\n",
        "        params.update({f\"W_q.{k}\": v for k, v in self.W_q.params().items()})\n",
        "        params.update({f\"W_k.{k}\": v for k, v in self.W_k.params().items()})\n",
        "        params.update({f\"W_v.{k}\": v for k, v in self.W_v.params().items()})\n",
        "        params.update({f\"W_o.{k}\": v for k, v in self.W_o.params().items()})\n",
        "        return params\n",
        "\n",
        "    def grads(self):\n",
        "        grads = {}\n",
        "        grads.update({f\"W_q.{k}\": v for k, v in self.W_q.grads().items()})\n",
        "        grads.update({f\"W_k.{k}\": v for k, v in self.W_k.grads().items()})\n",
        "        grads.update({f\"W_v.{k}\": v for k, v in self.W_v.grads().items()})\n",
        "        grads.update({f\"W_o.{k}\": v for k, v in self.W_o.grads().items()})\n",
        "        return grads\n",
        "\n"
      ],
      "metadata": {
        "id": "0G5LgToGjTdK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feed Forward"
      ],
      "metadata": {
        "id": "k5NDgZvwkuSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "        self.linear1 = Linear(d_model, d_ff)\n",
        "        self.relu = ReLU()\n",
        "        self.linear2 = Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = self.linear1(X)  # (B, T, d_ff)\n",
        "        X = self.relu(X)  # (B, T, d_ff)\n",
        "        X = self.linear2(X)  # (B, T, d_model)\n",
        "        return X\n",
        "\n",
        "    def backward(self, dZ):\n",
        "        dZ = self.linear2.backward(dZ)\n",
        "        dZ = self.relu.backward(dZ)\n",
        "        dZ = self.linear1.backward(dZ)\n",
        "        return dZ\n",
        "\n",
        "    def params(self):\n",
        "        params = {}\n",
        "        params.update({f\"linear1.{k}\": v for k, v in self.linear1.params().items()})\n",
        "        params.update({f\"linear2.{k}\": v for k, v in self.linear2.params().items()})\n",
        "        return params\n",
        "\n",
        "    def grads(self):\n",
        "        grads = {}\n",
        "        grads.update({f\"linear1.{k}\": v for k, v in self.linear1.grads().items()})\n",
        "        grads.update({f\"linear2.{k}\": v for k, v in self.linear2.grads().items()})\n",
        "        return grads"
      ],
      "metadata": {
        "id": "bUPdqpwhktrN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Linear"
      ],
      "metadata": {
        "id": "_cmOx2sawlWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear(Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        scale = np.sqrt(2.0 / in_features)\n",
        "\n",
        "        self.W = np.random.randn(in_features, out_features) * scale\n",
        "        self.b = np.zeros(out_features)\n",
        "\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "        self.cache_input = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.cache_input = X\n",
        "\n",
        "        if X.ndim == 3:\n",
        "            B, T, C = X.shape\n",
        "            X_reshaped = X.reshape(-1, C)  # (B*T, C)\n",
        "            out = X_reshaped @ self.W + self.b  # (B*T, out_features)\n",
        "            out = out.reshape(B, T, self.out_features)  # (B, T, out_features)\n",
        "            return out\n",
        "        else:\n",
        "            out = X @ self.W + self.b  # (B, out_features)\n",
        "            return out\n",
        "\n",
        "    def backward(self, dZ):\n",
        "        X = self.cache_input\n",
        "\n",
        "        if X.ndim == 3:\n",
        "            B, T, C = X.shape\n",
        "            X_reshaped = X.reshape(-1, C)\n",
        "            dZ_reshaped = dZ.reshape(-1, self.out_features)\n",
        "\n",
        "            # Y = XW + b, so ∂Y/∂W = X^T, ∂Y/∂b = I, ∂Y/∂X = W^T\n",
        "            self.dW = X_reshaped.T @ dZ_reshaped  # ∂L/∂W = X^T @ ∂L/∂Y\n",
        "            self.db = np.sum(dZ_reshaped, axis=0)  # ∂L/∂b = Σ ∂L/∂Y\n",
        "\n",
        "            dX_reshaped = dZ_reshaped @ self.W.T  # ∂L/∂X = ∂L/∂Y @ W^T\n",
        "            dX = dX_reshaped.reshape(B, T, C)\n",
        "            return dX\n",
        "        else:\n",
        "            self.dW = X.T @ dZ  # ∂L/∂W = X^T @ ∂L/∂Y\n",
        "            self.db = np.sum(dZ, axis=0)  # ∂L/∂b = Σ ∂L/∂Y\n",
        "            return dZ @ self.W.T  # ∂L/∂X = ∂L/∂Y @ W^T\n",
        "\n",
        "    def params(self):\n",
        "        return {\"W\": self.W, \"b\": self.b}\n",
        "\n",
        "    def grads(self):\n",
        "        return {\"W\": self.dW, \"b\": self.db}"
      ],
      "metadata": {
        "id": "2Fa2Ao-SwmpZ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transformer Block"
      ],
      "metadata": {
        "id": "hzYar7E3soTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadAttention(d_model, n_heads)\n",
        "        self.ln1 = LayerNorm(d_model)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.ln2 = LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, X, mask=None):\n",
        "        ln1_out = self.ln1(X)  #Pre-Normalization\n",
        "        attn_out = self.attn(ln1_out, mask)\n",
        "        X = X + attn_out  #Residual Connection\n",
        "\n",
        "        ln2_out = self.ln2(X)  #Pre-Normalization\n",
        "        ffn_out = self.ffn(ln2_out)\n",
        "        X = X + ffn_out  #Residual Connection\n",
        "\n",
        "        return X\n",
        "\n",
        "    def backward(self, dZ):\n",
        "        # Residual: y = x + f(x), so ∂L/∂x = ∂L/∂y * I + ∂L/∂f(x) · ∂f/∂x\n",
        "        # gradient flows through both paths\n",
        "        dffn_out = dZ.copy()\n",
        "        dX1 = dZ.copy()\n",
        "\n",
        "        dffn_in = self.ffn.backward(dffn_out)\n",
        "        dln2_out = dffn_in\n",
        "        dX2 = self.ln2.backward(dln2_out)\n",
        "\n",
        "        dX1 += dX2  # Sum both paths\n",
        "\n",
        "        dattn_out = dX1.copy()\n",
        "        dX3 = dX1.copy()\n",
        "\n",
        "        dattn_in = self.attn.backward(dattn_out)\n",
        "        dln1_out = dattn_in\n",
        "        dX4 = self.ln1.backward(dln1_out)\n",
        "\n",
        "        dX3 += dX4  # Sum both paths\n",
        "\n",
        "        return dX3\n",
        "\n",
        "    def params(self):\n",
        "        params = {}\n",
        "        params.update({f\"attn.{k}\": v for k, v in self.attn.params().items()})\n",
        "        params.update({f\"ln1.{k}\": v for k, v in self.ln1.params().items()})\n",
        "        params.update({f\"ffn.{k}\": v for k, v in self.ffn.params().items()})\n",
        "        params.update({f\"ln2.{k}\": v for k, v in self.ln2.params().items()})\n",
        "        return params\n",
        "\n",
        "    def grads(self):\n",
        "        grads = {}\n",
        "        grads.update({f\"attn.{k}\": v for k, v in self.attn.grads().items()})\n",
        "        grads.update({f\"ln1.{k}\": v for k, v in self.ln1.grads().items()})\n",
        "        grads.update({f\"ffn.{k}\": v for k, v in self.ffn.grads().items()})\n",
        "        grads.update({f\"ln2.{k}\": v for k, v in self.ln2.grads().items()})\n",
        "        return grads"
      ],
      "metadata": {
        "id": "ISf4BM3Hsqyt"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MiniGPT"
      ],
      "metadata": {
        "id": "3DbhMNN4wjQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class miniGPT(Module):\n",
        "\n",
        "    def __init__(self, vocab_size, max_len, d_model, n_heads, n_layers, d_ff):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.tok_emb = Embedding(vocab_size, d_model)\n",
        "        self.pos_emb = PositionalEncoding(max_len, d_model)\n",
        "        self.blocks = [TransformerBlock(d_model, n_heads, d_ff) for _ in range(n_layers)]\n",
        "        self.ln_f = LayerNorm(d_model)\n",
        "        self.lm_head = Linear(d_model, vocab_size) #linear layer that as the language modeling head\n",
        "\n",
        "        self.cache = {}\n",
        "\n",
        "    def forward(self, X, targets=None):\n",
        "        B, T = X.shape\n",
        "\n",
        "        tok_emb = self.tok_emb(X)  # (B, T, C)\n",
        "        X = self.pos_emb(tok_emb)  # (B, T, C)\n",
        "\n",
        "        mask = self._create_causal_mask(T)  # (1, 1, T, T)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            X = block(X, mask)  # (B, T, C)\n",
        "\n",
        "        X = self.ln_f(X)  # (B, T, C)\n",
        "        logits = self.lm_head(X)  # logits over vocab\n",
        "\n",
        "        if targets is not None:\n",
        "            loss = cross_entropy_loss(logits.reshape(-1, self.vocab_size), targets.reshape(-1))\n",
        "            self.cache = {'logits': logits, 'targets': targets}\n",
        "            return logits, loss\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def backward(self):\n",
        "        logits, targets = self.cache['logits'], self.cache['targets']\n",
        "        B, T, C = logits.shape\n",
        "\n",
        "        logits_reshaped = logits.reshape(-1, C)  # (B*T, C)\n",
        "        targets_reshaped = targets.reshape(-1)  # (B*T)\n",
        "\n",
        "        logits_max = np.max(logits_reshaped, axis=-1, keepdims=True)  # (B*T, 1)\n",
        "        exp_logits = np.exp(logits_reshaped - logits_max)  # for numerically stable softmax\n",
        "        probs = exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)  # (B*T, C)\n",
        "\n",
        "        # CE + softmax trick (https://stats.stackexchange.com/questions/235528/backpropagation-with-softmax-cross-entropy)\n",
        "        dlogits = probs\n",
        "        dlogits[np.arange(len(targets_reshaped)), targets_reshaped] -= 1  # d(CE)/d(logits) = softmax - one_hot\n",
        "        dlogits /= len(targets_reshaped)  # average over batch and sequence length\n",
        "\n",
        "        dlogits = dlogits.reshape(B, T, C)\n",
        "\n",
        "        dX = self.lm_head.backward(dlogits)\n",
        "        dX = self.ln_f.backward(dX)\n",
        "\n",
        "        for block in reversed(self.blocks):\n",
        "            dX = block.backward(dX)\n",
        "\n",
        "        dX = self.pos_emb.backward(dX)\n",
        "        self.tok_emb.backward(dX)\n",
        "\n",
        "    def _create_causal_mask(self, T):\n",
        "        mask = np.triu(np.ones((T, T)), k=1) * -1e9  # will add a large negative value to the scores of future tokens\n",
        "        return mask[None, None, :, :]\n",
        "\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, eos_token_id=None):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -self.max_len:]\n",
        "            logits = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "\n",
        "            logits_shifted = logits - np.max(logits, axis=-1, keepdims=True)  # for numerically stable softmax\n",
        "            probs = np.exp(logits_shifted) / np.sum(np.exp(logits_shifted), axis=-1, keepdims=True)\n",
        "\n",
        "            idx_next = np.array([[np.random.choice(self.vocab_size, p=probs[0])]])\n",
        "            idx = np.concatenate([idx, idx_next], axis=1)\n",
        "\n",
        "            if eos_token_id is not None and idx_next[0, 0] == eos_token_id:\n",
        "                break\n",
        "\n",
        "        return idx\n",
        "\n",
        "    def params(self):\n",
        "        params = {}\n",
        "        params.update({f\"tok_emb.{k}\": v for k, v in self.tok_emb.params().items()})\n",
        "        params.update({f\"pos_emb.{k}\": v for k, v in self.pos_emb.params().items()})\n",
        "\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            params.update({f\"blocks.{i}.{k}\": v for k, v in block.params().items()})\n",
        "\n",
        "        params.update({f\"ln_f.{k}\": v for k, v in self.ln_f.params().items()})\n",
        "        params.update({f\"lm_head.{k}\": v for k, v in self.lm_head.params().items()})\n",
        "        return params\n",
        "\n",
        "    def grads(self):\n",
        "        grads = {}\n",
        "        grads.update({f\"tok_emb.{k}\": v for k, v in self.tok_emb.grads().items()})\n",
        "        grads.update({f\"pos_emb.{k}\": v for k, v in self.pos_emb.grads().items()})\n",
        "\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            grads.update({f\"blocks.{i}.{k}\": v for k, v in block.grads().items()})\n",
        "\n",
        "        grads.update({f\"ln_f.{k}\": v for k, v in self.ln_f.grads().items()})\n",
        "        grads.update({f\"lm_head.{k}\": v for k, v in self.lm_head.grads().items()})\n",
        "        return grads"
      ],
      "metadata": {
        "id": "GU3FxhaJs0kW"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Implementation"
      ],
      "metadata": {
        "id": "9_lOas8Qt-tl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Word Tokenizer"
      ],
      "metadata": {
        "id": "JwrN7IQPu62z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(input_path) as f:\n",
        "    text = f.read()\n",
        "\n",
        "tokenizer = WordTokenizer(min_freq=1, max_vocab_size=1000)\n",
        "tokenizer.build_vocab(text)\n",
        "\n",
        "encoded = tokenizer.encode(text)\n",
        "\n",
        "split_idx = int(0.9 * len(encoded))\n",
        "train_data = np.array(encoded[:split_idx], dtype=np.int32)\n",
        "val_data = np.array(encoded[split_idx:], dtype=np.int32)\n",
        "\n",
        "np.save(output_path+'_train.npy', train_data)\n",
        "np.save(output_path+'_val.npy', val_data)\n",
        "\n",
        "with open(output_path+'_tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n"
      ],
      "metadata": {
        "id": "1XU1ApqHu6aL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = np.load(output_path + '_train.npy')\n",
        "val_data = np.load(output_path + '_val.npy')\n",
        "with open(output_path + '_tokenizer.pkl', 'rb') as f:\n",
        "    tokenizer = pickle.load(f)\n",
        "\n",
        "def get_batch(data, block_size, batch_size):\n",
        "    ix = np.random.randint(0, len(data) - block_size - 1, (batch_size,))\n",
        "    x = np.stack([data[i:i+block_size] for i in ix])\n",
        "    y = np.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "J6kKV0OUvhVu"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = tokenizer.vocab_size\n",
        "block_size = 64        # sequence length for each sample\n",
        "batch_size = 32        # mini-batch size\n",
        "d_model = 128\n",
        "n_heads = 4\n",
        "n_layers = 2\n",
        "d_ff = 512\n",
        "max_len = block_size   # maximum sequence length"
      ],
      "metadata": {
        "id": "nN37gQDEvvKR"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_word = miniGPT(\n",
        "    vocab_size=vocab_size,\n",
        "    max_len=max_len,\n",
        "    d_model=d_model,\n",
        "    n_heads=n_heads,\n",
        "    n_layers=n_layers,\n",
        "    d_ff=d_ff)"
      ],
      "metadata": {
        "id": "iCbMGuDZygLW"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "num_iters = 500  # how many mini-batches to run"
      ],
      "metadata": {
        "id": "1MhyX81iyiqb"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for it in range(num_iters):\n",
        "    # get a batch\n",
        "    x, y = get_batch(train_data, block_size, batch_size)\n",
        "\n",
        "    # forward pass\n",
        "    logits, loss = model_word.forward(x, targets=y)\n",
        "\n",
        "    # backward pass\n",
        "    model_word.backward()\n",
        "\n",
        "    # SGD update\n",
        "    for name, param in model_word.params().items():\n",
        "        grad = model_word.grads()[name]\n",
        "        param -= learning_rate * grad  # simple SGD\n",
        "\n",
        "    if it % 50 == 0:\n",
        "        print(f\"iter {it}, loss {loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yST-b8D0yoyw",
        "outputId": "f2798566-f91e-43ad-f180-2c6150569914"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 0, loss 8.0009\n",
            "iter 50, loss 7.2585\n",
            "iter 100, loss 6.8730\n",
            "iter 150, loss 6.6689\n",
            "iter 200, loss 6.6148\n",
            "iter 250, loss 6.6308\n",
            "iter 300, loss 6.3698\n",
            "iter 350, loss 6.5196\n",
            "iter 400, loss 6.2249\n",
            "iter 450, loss 6.4731\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = np.array([[tokenizer.encode(\"my dear\")[0]]])  # initial token(s)\n",
        "generated_ids = model_word.generate(context, max_new_tokens=20)\n",
        "print(tokenizer.decode(generated_ids[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zo6Bv5f7ywY2",
        "outputId": "1c71406d-c706-4499-c244-982083abfcd4"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "banished make half keeper; liege patience with pray brief know kill 3 been speed, become consul thyself late\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##BPE Tokenizer"
      ],
      "metadata": {
        "id": "gpoDNZW111Wt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(input_path) as f:\n",
        "    text = f.read()\n",
        "\n",
        "bpe_tokenizer = BPETokenizer(vocab_size=1000)  # adjust vocab size if needed\n",
        "bpe_tokenizer.build_vocab(text)\n",
        "\n",
        "bpe_encoded = bpe_tokenizer.encode(text)\n",
        "\n",
        "split_idx = int(0.9 * len(bpe_encoded))\n",
        "train_data = np.array(bpe_encoded[:split_idx], dtype=np.int32)\n",
        "val_data = np.array(bpe_encoded[split_idx:], dtype=np.int32)\n",
        "\n",
        "np.save(output_path + '_train_bpe.npy', train_data)\n",
        "np.save(output_path + '_val_bpe.npy', val_data)\n",
        "\n",
        "with open(output_path + '_tokenizer_bpe.pkl', 'wb') as f:\n",
        "    pickle.dump(bpe_tokenizer, f)\n"
      ],
      "metadata": {
        "id": "2E6IhDtc10jf"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = np.load(output_path + '_train_bpe.npy')\n",
        "val_data = np.load(output_path + '_val_bpe.npy')\n",
        "with open(output_path + '_tokenizer_bpe.pkl', 'rb') as f:\n",
        "    bpe_tokenizer = pickle.load(f)"
      ],
      "metadata": {
        "id": "FfWXmmNo34Br"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = bpe_tokenizer.vocab_size\n",
        "block_size = 64\n",
        "batch_size = 32\n",
        "d_model = 128\n",
        "n_heads = 4\n",
        "n_layers = 2\n",
        "d_ff = 512\n",
        "max_len = block_size"
      ],
      "metadata": {
        "id": "it3O0fso38B3"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_bpe = miniGPT(\n",
        "    vocab_size=vocab_size,\n",
        "    max_len=max_len,\n",
        "    d_model=d_model,\n",
        "    n_heads=n_heads,\n",
        "    n_layers=n_layers,\n",
        "    d_ff=d_ff\n",
        ")"
      ],
      "metadata": {
        "id": "u0YJg9or3_Ci"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "num_iters = 500  # mini-batches"
      ],
      "metadata": {
        "id": "l2_2Ffn44C2Q"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for it in range(num_iters):\n",
        "    # get a batch\n",
        "    x, y = get_batch(train_data, block_size, batch_size)\n",
        "\n",
        "    # forward pass\n",
        "    logits, loss = model_bpe.forward(x, targets=y)\n",
        "\n",
        "    # backward pass\n",
        "    model_bpe.backward()\n",
        "\n",
        "    # SGD update\n",
        "    for name, param in model_bpe.params().items():\n",
        "        grad = model_bpe.grads()[name]\n",
        "        param -= learning_rate * grad  # simple SGD\n",
        "\n",
        "    if it % 50 == 0:\n",
        "        print(f\"iter {it}, loss {loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7C6yjEL4F2q",
        "outputId": "8b0e9076-ecbb-4416-9092-2b0fef89e31b"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 0, loss 7.8874\n",
            "iter 50, loss 7.2187\n",
            "iter 100, loss 6.9284\n",
            "iter 150, loss 6.7447\n",
            "iter 200, loss 6.4249\n",
            "iter 250, loss 6.3946\n",
            "iter 300, loss 6.3722\n",
            "iter 350, loss 6.3591\n",
            "iter 400, loss 6.2878\n",
            "iter 450, loss 6.1805\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = np.array([[bpe_tokenizer.encode(\"the\")[0]]])  # initial token(s)\n",
        "generated_ids = model_bpe.generate(context, max_new_tokens=20, eos_token_id=bpe_tokenizer.eos_token_id)\n",
        "print(bpe_tokenizer.decode(generated_ids[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SktSC6NG4J_d",
        "outputId": "e871e3c0-55ee-4c67-ef31-d52739e75b2b"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the: Then itizISABELL, LIan' IANrionce mber two dead GREDUKE chard you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing"
      ],
      "metadata": {
        "id": "9ZVt19PkMoi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Running simple verification tests...\")\n",
        "\n",
        "#dummy\n",
        "test_vocab_size = 50\n",
        "test_block_size = 8\n",
        "test_batch_size = 2\n",
        "test_d_model = 16\n",
        "test_n_heads = 2\n",
        "test_d_ff = 32\n",
        "test_n_layers = 1\n",
        "\n",
        "#uji dimensi\n",
        "print(\"\\n--- Testing Model Output Dimensions ---\")\n",
        "try:\n",
        "    # dummy model\n",
        "    dummy_model = miniGPT(\n",
        "        vocab_size=test_vocab_size,\n",
        "        max_len=test_block_size,\n",
        "        d_model=test_d_model,\n",
        "        n_heads=test_n_heads,\n",
        "        n_layers=test_n_layers,\n",
        "        d_ff=test_d_ff\n",
        "    )\n",
        "    dummy_input = np.random.randint(0, test_vocab_size, (test_batch_size, test_block_size))\n",
        "\n",
        "    # Lakukan forward pass\n",
        "    logits = dummy_model(dummy_input)\n",
        "\n",
        "    # Check dimension output\n",
        "    expected_shape = (test_batch_size, test_block_size, test_vocab_size)\n",
        "    assert logits.shape == expected_shape, f\"Shape mismatch! Expected {expected_shape}, got {logits.shape}\"\n",
        "    print(f\"Test Passed: Output shape is correct {logits.shape}.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Test Failed: {e}\")\n",
        "\n",
        "\n",
        "# test softmax properties\n",
        "print(\"\\n--- Testing Softmax Properties ---\")\n",
        "try:\n",
        "    logits_last_step = logits[:, -1, :] # logits from last token\n",
        "\n",
        "    # re-implement softmax for verification\n",
        "    exp_logits = np.exp(logits_last_step - np.max(logits_last_step, axis=-1, keepdims=True))\n",
        "    probs = exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)\n",
        "\n",
        "    # check every row\n",
        "    assert np.allclose(np.sum(probs, axis=-1), 1.0), \"Softmax sums are not close to 1.\"\n",
        "    print(\"Test Passed: Probabilities sum to 1.\")\n",
        "\n",
        "    # check probability\n",
        "    assert np.all(probs >= 0) and np.all(probs <= 1), \"Probabilities are not between 0 and 1.\"\n",
        "    print(\"Test Passed: All probability values are within the [0, 1] range.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Test Failed: {e}\")\n",
        "\n",
        "\n",
        "# causal mask\n",
        "print(\"\\n--- Testing Causal Masking ---\")\n",
        "try:\n",
        "    # initiate mha\n",
        "    mha = MultiHeadAttention(d_model=test_d_model, n_heads=test_n_heads)\n",
        "\n",
        "    # dummy\n",
        "    dummy_mha_input = np.random.randn(test_batch_size, test_block_size, test_d_model)\n",
        "\n",
        "    # get Q, K, dan V\n",
        "    Q = mha.W_q(dummy_mha_input).reshape(test_batch_size, test_block_size, test_n_heads, mha.d_k).swapaxes(1, 2)\n",
        "    K = mha.W_k(dummy_mha_input).reshape(test_batch_size, test_block_size, test_n_heads, mha.d_k).swapaxes(1, 2)\n",
        "    V = mha.W_v(dummy_mha_input).reshape(test_batch_size, test_block_size, test_n_heads, mha.d_k).swapaxes(1, 2)\n",
        "\n",
        "    # calculate score\n",
        "    scores = Q @ K.swapaxes(-2, -1) / np.sqrt(mha.d_k)\n",
        "\n",
        "    # create and implement causal mask\n",
        "    causal_mask = np.triu(np.ones((test_block_size, test_block_size)), k=1) * -1e9\n",
        "    masked_scores = scores + causal_mask\n",
        "\n",
        "    # softmax\n",
        "    softmax_layer = Softmax()\n",
        "    attn_weights = softmax_layer(masked_scores.reshape(-1, test_block_size)).reshape(scores.shape)\n",
        "\n",
        "    # check future attention weight\n",
        "    first_attn_matrix = attn_weights[0, 0, :, :]\n",
        "\n",
        "    # get upper triangle\n",
        "    future_attention_weights = np.triu(first_attn_matrix, k=1)\n",
        "\n",
        "    assert np.allclose(future_attention_weights, 0), \"Causal mask failed! Model is attending to future tokens.\"\n",
        "    print(\"Test Passed: Causal mask correctly prevents attention to future tokens.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Test Failed: {e}\")\n",
        "\n",
        "print(\"\\nAll simple tests completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3E7yMr5MpxE",
        "outputId": "3c46468e-de24-4934-f006-1086edb8b581"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running simple verification tests...\n",
            "\n",
            "--- Testing Model Output Dimensions ---\n",
            "Test Passed: Output shape is correct (2, 8, 50).\n",
            "\n",
            "--- Testing Softmax Properties ---\n",
            "Test Passed: Probabilities sum to 1.\n",
            "Test Passed: All probability values are within the [0, 1] range.\n",
            "\n",
            "--- Testing Causal Masking ---\n",
            "Test Passed: Causal mask correctly prevents attention to future tokens.\n",
            "\n",
            "All simple tests completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Credits\n",
        "1. [Attention is All You Need](https://arxiv.org/abs/1706.03762)\n",
        "2. [numpyGPT](https://github.com/codiceSpaghetti/numpyGPT)\n",
        "3. ChatGPT as my personal assistant who help me understand the attention mechanism :D and this assignment"
      ],
      "metadata": {
        "id": "asPrgke947Gn"
      }
    }
  ]
}